---
layout: post
title: Scrapy初探
tag: Spider
---

### 创建scrapy项目
```
# 安装scrapy
$ pip install scrapy

# 进入要创建项目的文件夹
# shift + 右键 在当前文件夹打开命令窗口

# 需要提前配置好python的环境变量
$ scrapy startproject mySpider

# scrapy command will look like this...
$ scrapy <command> [options] [args]
```
### scrapy的项目结构
```
mySpider/
   scrapy.cfg            # 项目的配置文件
   mySpider/
       __init__.py
       items.py
       pipelines.py
       settings.py
       spiders/          # 用户自定义的spider放在spiders目录下面
           __init__.py
           ...
```

### 创建一个Scrapy Spider
```
# 使用genspider命令创建一个名为jobs的spider，要爬去的网址是 https://newyork.craigslist.org/search/egr
$ scrapy genspider jobs https://newyork.craigslist.org/search/egr
```
> 注意事项：Scrapy 在生成时，会在`start_urls`中自动添加前缀`http://`和后缀`/`，在使用`scrapy genspider jobs https://newyork.craigslist.org/search/egr`命令的时候，因为我们的`args`中已经添加了`https://`，这样就会冲突，在写爬虫的时候需要再三检查，需要把scrapy默认添加的前缀`http://`删掉。

生成的文件路径：mySpider/mySpider/spiders/jobs.py
```python
# -*- coding: utf-8 -*-
import scrapy
class JobsSpider(scrapy.Spider):
    # 爬虫的名字，不能和其他的爬虫重名
    name = "jobs" 
    # 允许爬虫爬取的域名列表
    allowed_domains = ["craigslist.org"] 
    # 爬虫开始爬行的url列表
    start_urls = ['https://newyork.craigslist.org/search/egr/']

    # 爬虫的main方法，不可以改变方法名，不过可以添加其他的方法
    def parse(self, response):
        pass

```

### 写爬虫逻辑
```
# 将上一步parse方法中的pass去掉
def parse(self, response):
    
    # title 是基于xpath规则提取的文本部分
    # response 是爬取的页面的html源码，另外使用print(reponse)，你可以得
    # 到类似<200 https://newyork.craigslist.org/search/egr> ，这意味
    # 着已经成功的连接到这个网页。
    # response 还可以使用css选择器的方式选择要爬取的数据，
    # response.css("css selector")
    # xpath的含义：提取<a>标签中的文本'Chief Engineer'
    # extract() 将提取相同XPath规则的网页上的每个实例。
    # extract_first() 将取列表中的第一个实例
    titles = response.xpath('//a[@class="result-title hdrlnk"]/text()').extract()
    #  the result is a list of Unicode strings,So you can loop on them, and yield one title per time in a form of dictionary.
    for title in titles:
        yield {'Title': title}
```
　　上面的xpath是爬取如下的文本数据
```
<a href="/brk/egr/6085878649.html" data-id="6085878649" class="result-title hdrlnk">Chief Engineer</a>
```
### XPath表达式语法：
* `nodename`：选取此节点的所有子节点
* `/`：从根节点选取
* `//`：从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置
* `.`：选取当前的节点
* `..`：选取当前节点的父节点
* `@`：选取属性

XML实例文档

```
<?xml version="1.0" encoding="ISO-8859-1"?>

<bookstore>

    <book>
      <title lang="eng">Harry Potter</title>
      <price>29.99</price>
    </book>

    <book>
      <title lang="eng">Learning XML</title>
      <price>39.95</price>
    </book>

</bookstore>
```

XPath表达式的写法：

| 路径表达式 | 结果 |
| :--------: | :--------: |
| bookstore | 选取 bookstore 元素的所有子节点 |
| /bookstore | 选取根元素 bookstore <br/> 注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！|
| bookstore/book | 选取属于 bookstore 的子元素的所有 book 元素 |
| //book | 选取所有 book 子元素，而不管它们在文档中的位置 |
| bookstore//book | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置 |
| //@lang | 选取名为 lang 的所有属性 |

### 运行爬虫
```
$ scrapy crawl jobs
```

### 将爬取的数据存到csv文件中
```
$ scrapy crawl jobs -o result-titles.csv
```
Your Terminal should show you a similar result, which indicates success
```
'downloader/response_status_count/200': 2,
'finish_reason': 'finished',
'finish_time': datetime.datetime(2017, 5, 2, 17, 26, 30, 348412),
'item_scraped_count': 120,
'log_count/DEBUG': 123,
'log_count/INFO': 8,
```
`item_scraped_count` refers to the number of titles scraped from the page. `log_count/DEBUG`  and `log_count/INFO` are okay; however, if you received 'log_count/ERROR' you should find out which errors you get during scraping are fix your code.

In Terminal, you will notice debug messages like:
```
DEBUG: Scraped from <200 https://newyork.craigslist.org/search/egr>
```
The status code 200 means the request has succeeded. Also,  `downloader/response_status_count/200` tells you how many requests succeeded. There are many other [status codes](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html) with different meanings; however, in web scraping they could act as a defense mechanism against web scraping.