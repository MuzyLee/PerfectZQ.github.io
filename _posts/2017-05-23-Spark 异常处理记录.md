---
layout: post
title: Spark 异常处理记录
tag: Spark
---

# Spark 异常处理记录
### spark程序运行空指针异常的原因
1.嵌套使用了RDD操作，比如在一个RDD map中又对另一个RDD进行了map操作。主要原因在于spark不支持RDD的嵌套操作。

2.在RDD操作中引用了object非原始类型(非int long等简单类型)的成员变量。貌似是由于object的成员变量默认是无法序列化的。解决方法：可以先将成员变量赋值给一个临时变量，然后使用该临时变量即可

3.spark 2.0.0对kryo序列化的依赖有bug，到SPARK_HOME/conf/spark-defaults.conf
将默认： spark.serializer     org.apache.spark.serializer.KryoSerializer
改为：	 spark.serializer 	org.apache.spark.serializer.JavaSerializer
### java.io.NotSerializableException: org.apache.spark.SparkContext
sparkContext对象是不能被序列化的，sparkContext不可以出现在map函数中
### object not serializable(class:org.apache.hadoop.hbase.io.ImmutableBytesWritable)
Spark操作HBase返回的是 RDD[ImmutableWritable,Result]类型，当在集群上对此RDD进行操作的时候。（比如join），就会产生此异常，因为org.apache.hadoop.hbase.io.ImmutableBytesWritable和org.apache.hadoop.hbase.client.Result并没有实现java.io.Serializable接口

解决方式：

方式一：
```
// 手动设置如何序列化ImmutableWritable类
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```
方式二：
将ImmutableWritable转换成其他可序列化的类
将其中的数据抽取出来放在可以序列化的类中，比如String或者数组

### 使用 Spark ML 决策树构建 DataFrame 的时候出现下面的异常：java.lang.IllegalArgumentException: requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.
解决办法：
```
import org.apache.spark.mllib.linalg.Vectors
=>
import org.apache.spark.ml.linalg.Vectors
```
### 运行 Spark ML 决策树的时候出现 java.lang.OutOfMemoryError: PermGen space （永生代的内存溢出）

解决方法：
在IDEA的run configuration中添加VM options  -XX:PermSize=128m

原因：
PermGen space用于存放Class和Meta的信息,Class在被 Load的时候被放入PermGen space区域，它和和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话,就很可能
出现PermGen space错误。这种错误常见在web服务器对JSP进行pre compile的时候。

改正方法：
```
-Xms256m -Xmx256m -XX:MaxNewSize=256m -XX:MaxPermSize=256m 
```

